{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637dd50f-2069-4c9c-91d6-a4210344223f",
   "metadata": {},
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6799f8f-b966-44e2-a7f8-57b2df5dc57c",
   "metadata": {},
   "source": [
    "If a model while getting trained gives high accuracy, but gives low accuracy on the test dataset then the model is said to be overfitting.Then the model does not categorize the data correctly, because of too many details and noise.         \n",
    "It can be mitigated by :                           \n",
    "1. Increase training data.                                \n",
    "2. Reduce model complexity.                                                   \n",
    "3. Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).                                                          \n",
    "4. Ridge Regularization and Lasso Regularization.                                                       \n",
    "5. Use dropout for neural networks to tackle overfitting.                                                         \n",
    "If a model while getting trained gives low accuracy, also gives low accuracy on the test dataset then the model is said to be underfitting.The model cannot categorise the data correctly because it is not traines properly.                        \n",
    "It can be mitigated by :                                       \n",
    "1. Increase model complexity.                                             \n",
    "2. Increase the number of features, performing feature engineering.                                       \n",
    "3. Remove noise from the data.                                                                            \n",
    "4. Increase the number of epochs or increase the duration of training to get better results.                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514d2c4-b7ce-4c41-8f1a-6b3a2c371625",
   "metadata": {},
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb127097-35b9-4857-81cf-368ea9772c5c",
   "metadata": {},
   "source": [
    "1. Hold-out (data)                                                  \n",
    "Rather than using all of our data for training, we can simply split our dataset into two sets: training and testing. A common split ratio is 80% for training and 20% for testing. We train our model until it performs well not only on the training set but also for the testing set. This indicates good generalization capability since the testing set represents unseen data that were not used for training. However, this approach would require a sufficiently large dataset to train on even after splitting.                                            \n",
    "\n",
    "2. Cross-validation (data)                                                           \n",
    "We can split our dataset into k groups (k-fold cross-validation). We let one of the groups to be the testing set (please see hold-out explanation) and the others as the training set, and repeat this process until each individual group has been used as the testing set (e.g., k repeats). Unlike hold-out, cross-validation allows all data to be eventually used for training but is also more computationally expensive than hold-out.\n",
    "\n",
    "\n",
    "3. Data augmentation (data)                                                                                \n",
    "A larger dataset would reduce overfitting. If we cannot gather more data and are constrained to the data we have in our current dataset, we can apply data augmentation to artificially increase the size of our dataset. For example, if we are training for an image classification task, we can perform various image transformations to our image dataset (e.g., flipping, rotating, rescaling, shifting).\n",
    "\n",
    "\n",
    "4. Feature selection (data)                                                    \n",
    "If we have only a limited amount of training samples, each with a large number of features, we should only select the most important features for training so that our model doesn’t need to learn for so many features and eventually overfit. We can simply test out different features, train individual models for these features, and evaluate generalization capabilities, or use one of the various widely used feature selection methods.\n",
    "\n",
    "\n",
    "5. L1 / L2 regularization (learning algorithm)                                                            \n",
    "Regularization is a technique to constrain our network from learning a model that is too complex, which may therefore overfit. In L1 or L2 regularization, we can add a penalty term on the cost function to push the estimated coefficients towards zero (and not take more extreme values). L2 regularization allows weights to decay towards zero but not to zero, while L1 regularization allows weights to decay to zero.\n",
    "\n",
    "\n",
    "6. Remove layers / number of units per layer (model)                                                            \n",
    "As mentioned in L1 or L2 regularization, an over-complex model may more likely overfit. Therefore, we can directly reduce the model’s complexity by removing layers and reduce the size of our model. We may further reduce complexity by decreasing the number of neurons in the fully-connected layers. We should have a model with a complexity that sufficiently balances between underfitting and overfitting for our task.\n",
    "\n",
    "\n",
    "7. Dropout (model)                                                                    \n",
    "By applying dropout, which is a form of regularization, to our layers, we ignore a subset of units of our network with a set probability. Using dropout, we can reduce interdependent learning among units, which may have led to overfitting. However, with dropout, we would need more epochs for our model to converge.\n",
    "\n",
    "\n",
    "8. Early stopping (model)                                                                    \n",
    "We can first train our model for an arbitrarily large number of epochs and plot the validation loss graph (e.g., using hold-out). Once the validation loss begins to degrade (e.g., stops decreasing but rather begins increasing), we stop the training and save the current model. We can implement this either by monitoring the loss graph or set an early stopping trigger. The saved model would be the optimal model for generalization among different training epoch values.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16321e5f-d316-4918-817a-46f44444a5be",
   "metadata": {},
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f09ec13-5418-4467-9e93-9b54672900ba",
   "metadata": {},
   "source": [
    "If a model while getting trained gives low accuracy, also gives low accuracy on the test dataset then the model is said to be underfitting.\n",
    "scenarios where underfitting can occur in ML are :\n",
    "Data used for training is not cleaned and contains noise (garbage values) in it.\n",
    "The model has a high bias.\n",
    "The size of the training dataset used is not enough.\n",
    "The model is too simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec31ae9a-494d-47b7-bff8-c9e5adaad1f0",
   "metadata": {},
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73188d6-a2cb-4286-8e72-c9797122568f",
   "metadata": {},
   "source": [
    "If our model is too simple and has very few parameters then it may have high bias and low variance. On the other hand if our model has large number of parameters then it’s going to have high variance and low bias. So we need to find the right/good balance without overfitting and underfitting the data.                                                          \n",
    "\n",
    "This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time.                                                              \n",
    "Bias and variance are inversely connected. It is impossible to have an ML model with a low bias and a low variance. When a data engineer modifies the ML algorithm to better fit a given data set, it will lead to low bias—but it will increase variance.The model performance gets low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526012ce-c096-4c79-9b0a-fb126ba9bc87",
   "metadata": {},
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8965928-37d8-45fc-916d-19476fac8555",
   "metadata": {},
   "source": [
    "Learning curves are a great tool to help us determine whether a model is overfitting or underfitting:                 \n",
    "An overfitting model performs well on the training data but doesn't generalize to testing data.                       \n",
    "An underfitting model performs poorly on training and testing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90600d56-2bd9-4cd7-9bc1-5746b33a920b",
   "metadata": {},
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf99bd66-511b-4400-a69a-1f108dd0a629",
   "metadata": {},
   "source": [
    "Variance specifies the amount of variation that the estimate of the target function will change if different training data was used. Bias refers to the difference between predicted values and actual values. Variance says about how much a random variable deviates from its expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2606ef9-34c9-48c1-9a8c-b68142214244",
   "metadata": {},
   "source": [
    "An algorithm with high bias is Linear Regression, Linear Discriminant Analysis and Logistic Regression.                       \n",
    "High-variance models include those that strongly rely on individual data points to define their parameters such as classification or regression trees, nearest neighbor models, and neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f713bb9-1505-4582-8f9f-4bb205a2577f",
   "metadata": {},
   "source": [
    "Differences in Performance:              \n",
    "\n",
    "High bias models tend to have low complexity and may have difficulty capturing intricate relationships in the data. As a result, they often underfit the training data and have high bias and high training error. However, these models may exhibit low variability and can generalize well to unseen data, resulting in lower testing error compared to high variance models.      \n",
    "High variance models, on the other hand, have high complexity and can capture complex patterns in the training data. They have low bias and can fit the training data very well, resulting in low training error. However, these models are more prone to overfitting and have high variability, leading to higher testing error compared to high bias models.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43de6d6d-9360-4d2e-a936-447aec0e7b16",
   "metadata": {},
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f998ae-f1ad-4e27-9d45-e584ce96c670",
   "metadata": {},
   "source": [
    "Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, avoiding the risk of Overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e397f1af-6b82-4edb-b8aa-12f33e9b94c5",
   "metadata": {},
   "source": [
    "Mainly, there are two types of regularization techniques, which are given below:                   \n",
    "Ridge Regression                                      \n",
    "Lasso Regression                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd44028-9349-4b88-84a1-96f180319433",
   "metadata": {},
   "source": [
    "Ridge Regression :                                \n",
    "                                            \n",
    "Ridge regression is one of the types of linear regression in which a small amount of bias is introduced so that we can get better long-term predictions.                                           \n",
    "Ridge regression is a regularization technique, which is used to reduce the complexity of the model. It is also called as L2 regularization.                                                               \n",
    "In this technique, the cost function is altered by adding the penalty term to it. The amount of bias added to the model is called Ridge Regression penalty. We can calculate it by multiplying with the lambda to the squared weight of each individual feature.                                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3883b74f-88df-4281-a166-318dfe8365c2",
   "metadata": {},
   "source": [
    "Lasso Regression:                                                                                                                                                                          \n",
    "Lasso regression is another regularization technique to reduce the complexity of the model. It stands for Least Absolute and Selection Operator.                                              \n",
    "It is similar to the Ridge Regression except that the penalty term contains only the absolute weights instead of a square of weights.                                                                                              \n",
    "Since it takes absolute values, hence, it can shrink the slope to 0, whereas Ridge Regression can only shrink it near to 0.\n",
    "It is also called as L1 regularization. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
